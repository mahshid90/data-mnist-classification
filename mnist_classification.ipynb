{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some images from the train set\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):  # Display first 10 images\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\") \n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test/ 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1) \n",
    "X_test = np.expand_dims(X_test, axis=-1)   \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First Convolutional + MaxPooling Layer\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=(4,4), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Second Convolutional + MaxPooling Layer\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully Connected (Dense) Layer\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    # Output Layer (Softmax for multiclass classification)\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:27:15.569616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.4035 - accuracy: 0.8740 - val_loss: 0.1789 - val_accuracy: 0.9463\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 10s 6ms/step - loss: 0.1427 - accuracy: 0.9577 - val_loss: 0.1094 - val_accuracy: 0.9678\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.0882 - val_accuracy: 0.9737\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.0878 - accuracy: 0.9731 - val_loss: 0.0832 - val_accuracy: 0.9753\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.0745 - accuracy: 0.9765 - val_loss: 0.0908 - val_accuracy: 0.9723\n",
      "Final validation accuracy: 0.9723333120346069\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train_cat, \n",
    "    epochs=5,  \n",
    "    batch_size=32,  \n",
    "    validation_split=0.2,  \n",
    "    callbacks=[early_stopping] \n",
    ")\n",
    "\n",
    "# Optionally, print the final validation accuracy after training\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9739999771118164"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat)\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
